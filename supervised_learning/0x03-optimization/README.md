# OPTIMIZATION
<img src="https://i.ibb.co/Y3QwnZ3/OPTI.jpg" alt="OPTI" border="0">

## LEARNING OBJECTIVES

<li>What is a hyperparameter?</li>
<li>How and why do you normalize your input data?</li>
<li>What is a saddle point?</li>
<li>What is stochastic gradient descent?</li>
<li>What is mini-batch gradient descent?</li>
<li>What is a moving average? How do you implement it?</li>
<li>What is gradient descent with momentum? How do you implement it?</li>
<li>What is RMSProp? How do you implement it?</li>
<li>What is Adam optimization? How do you implement it?</li>
<li>What is learning rate decay? How do you implement it?</li>
<li>What is batch normalization? How do you implement it?</li>

## AUTHORS
<li> Paula Fuentes </li>

